(window.webpackJsonp=window.webpackJsonp||[]).push([[39],{386:function(a,t,v){"use strict";v.r(t);var _=v(26),r=Object(_.a)({},(function(){var a=this,t=a.$createElement,v=a._self._c||t;return v("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[v("h1",{attrs:{id:"基本概念"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#基本概念"}},[a._v("#")]),a._v(" 基本概念")]),a._v(" "),v("h2",{attrs:{id:"机器学习-ml-machine-learning"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#机器学习-ml-machine-learning"}},[a._v("#")]),a._v(" 机器学习（ML, Machine Learning）")]),a._v(" "),v("p",[a._v("让机器学习一些数据然后建立模型的过程。")]),a._v(" "),v("ul",[v("li",[a._v("回归：预测一些数值，比如预测股价、经济增长速度等（监督学习）。")]),a._v(" "),v("li",[a._v("分类：对有标签的数据进行分类（监督学习）。")]),a._v(" "),v("li",[a._v("聚类：对无标签的数据进行分类（无监督学习）。")]),a._v(" "),v("li",[a._v("降维：用低维的模型表示高维的数据，保留基本属性。")])]),a._v(" "),v("h3",{attrs:{id:"模型"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#模型"}},[a._v("#")]),a._v(" 模型")]),a._v(" "),v("p",[a._v("模型就是输入输出的映射，可以理解为是一个函数或一个方法或一个公式。（比如，y = wx + b）")]),a._v(" "),v("h3",{attrs:{id:"特征-feature"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#特征-feature"}},[a._v("#")]),a._v(" 特征（Feature）")]),a._v(" "),v("p",[a._v("对应模型输入的 x，有多少特征就有多少输入（x1, x2, x3, ...），比如芒果的颜色、大小、重量等。")]),a._v(" "),v("h3",{attrs:{id:"标签-label"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#标签-label"}},[a._v("#")]),a._v(" 标签（Label）")]),a._v(" "),v("p",[a._v("对应模型输出的 y，比如芒果的成熟程度。")]),a._v(" "),v("h3",{attrs:{id:"参数"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#参数"}},[a._v("#")]),a._v(" 参数")]),a._v(" "),v("p",[a._v("对应模型的 w 和 b。")]),a._v(" "),v("h3",{attrs:{id:"监督学习"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#监督学习"}},[a._v("#")]),a._v(" 监督学习")]),a._v(" "),v("p",[a._v("对有标签的数据进行学习（告诉模型输入的 x 和对应输出的 y）。")]),a._v(" "),v("h3",{attrs:{id:"无监督学习"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#无监督学习"}},[a._v("#")]),a._v(" 无监督学习")]),a._v(" "),v("p",[a._v("对无标签的数据进行学习（只告诉模型输入的 x，不告诉输出的 y）。")]),a._v(" "),v("h3",{attrs:{id:"半监督学习"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#半监督学习"}},[a._v("#")]),a._v(" 半监督学习")]),a._v(" "),v("p",[a._v("对少量有标签和大量无标签的数据进行学习（告诉模型一部分数据输入的 x 和对应输出的 y）。")]),a._v(" "),v("h3",{attrs:{id:"强化学习"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#强化学习"}},[a._v("#")]),a._v(" 强化学习")]),a._v(" "),v("p",[a._v("对无标签的数据进行学习，然后对符合正确预期的学习进行奖励从而强化。")]),a._v(" "),v("h3",{attrs:{id:"深度学习"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#深度学习"}},[a._v("#")]),a._v(" 深度学习")]),a._v(" "),v("p",[a._v("使用多层神经网络模型进行学习。")]),a._v(" "),v("h3",{attrs:{id:"损失函数-误差函数-lost-function"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#损失函数-误差函数-lost-function"}},[a._v("#")]),a._v(" 损失函数 / 误差函数（Lost Function）")]),a._v(" "),v("p",[a._v("用于评价模型的好坏，定义在单个样本上，算的是一个样本的误差。")]),a._v(" "),v("ul",[v("li",[a._v("0 - 1 损失函数（预测正确，损失函数为 0；预测错误，损失函数为 1。）")]),a._v(" "),v("li",[a._v("平方损失函数（预测值与真实值差的平方）")]),a._v(" "),v("li",[a._v("绝对损失函数（预测值与真实值差的绝对值）")]),a._v(" "),v("li",[a._v("对数损失函数")])]),a._v(" "),v("h3",{attrs:{id:"代价函数-成本函数-cost-function"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#代价函数-成本函数-cost-function"}},[a._v("#")]),a._v(" 代价函数 / 成本函数（Cost Function）")]),a._v(" "),v("p",[a._v("用于在平均意义上评价模型的好坏，定义在所有样本上，算的是所有样本误差和的平均，代价函数 = 损失函数和的平均（期望）。")]),a._v(" "),v("ul",[v("li",[a._v("均方误差（MSE, Mean Squared Error）：平方损失函数和的平均，一般用于线性回归模型。")]),a._v(" "),v("li",[a._v("均方根误差（RMSE, Root Mean Squared Error）：均方误差的算术平方根。")]),a._v(" "),v("li",[a._v("平均绝对误差（MAE, Mean Absolute Error）：绝对损失函数和的平均。")]),a._v(" "),v("li",[a._v("交叉熵（Cross Entry）：一般用于逻辑回归模型。")])]),a._v(" "),v("h3",{attrs:{id:"目标函数-object-function"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#目标函数-object-function"}},[a._v("#")]),a._v(" 目标函数（Object Function）")]),a._v(" "),v("p",[a._v("用于评价模型的好坏，目标函数 = 代价函数（经验风险） + 正则化项（结构风险）。")]),a._v(" "),v("blockquote",[v("p",[a._v("很多时候，损失函数、代价函数、目标函数往往都不进行区分，都混为一谈。")])]),a._v(" "),v("h3",{attrs:{id:"风险函数-risk-function"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#风险函数-risk-function"}},[a._v("#")]),a._v(" 风险函数（Risk Function）")]),a._v(" "),v("ul",[v("li",[a._v("经验风险：决定模型的拟合程度，拟合程度越高，经验风险越小（样本容量不大的情况下，容易产生过拟合问题）。"),v("strong",[a._v("与代价函数有关。")])]),a._v(" "),v("li",[a._v("结构风险：决定模型的复杂程度，模型结构越简单，结构风险越小。"),v("strong",[a._v("与正则化项（也叫做惩罚项）有关。")])])]),a._v(" "),v("h3",{attrs:{id:"正则化"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#正则化"}},[a._v("#")]),a._v(" 正则化")]),a._v(" "),v("p",[a._v("即结构风险最小化。")]),a._v(" "),v("h3",{attrs:{id:"过拟合"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#过拟合"}},[a._v("#")]),a._v(" 过拟合")]),a._v(" "),v("p",[a._v("对数据拟合程度过高（经验风险小，结构风险大）。")]),a._v(" "),v("h3",{attrs:{id:"欠拟合"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#欠拟合"}},[a._v("#")]),a._v(" 欠拟合")]),a._v(" "),v("p",[a._v("对数据拟合程度过差（经验风险大，结构风险小）。")]),a._v(" "),v("h3",{attrs:{id:"优化器-optimizer"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#优化器-optimizer"}},[a._v("#")]),a._v(" 优化器（Optimizer）")]),a._v(" "),v("p",[a._v("解决机器学习中最优化（最优解）的问题，求解函数极值，包括极大值和极小值。")]),a._v(" "),v("p",[a._v("比如，可用来优化损失（代价）函数，求解损失（代价）函数最小损失（代价）的解。")]),a._v(" "),v("ul",[v("li",[a._v("梯度下降法（GD, Gradient Descent）：在有限视距内寻找最快的路径下山，因此每走一步，都要参考当前位置最陡的方向才迈出下一步。准确度较高，速度慢，并且容易陷入局部最优解。")]),a._v(" "),v("li",[a._v("随机梯度下降法（SGD, Stochastic Gradient Descent）：下山的时候随便乱下，虽然下山过程会显得扭扭曲曲，但是总能下到山底。准确度较低，速度快，并不是全局最优，不易于并行实现。")]),a._v(" "),v("li",[a._v("批量梯度下降法（BGD, Batch Gradient Descent）：在下山之前就掌握了整座山的地势情况，选择总体平均梯度最小的方向下山。准确度高，可得到全局最优解，易于并行实现，当样本的数目很多时，速度会变慢。")]),a._v(" "),v("li",[a._v("小批量梯度下降法（MBGD, Mini-Batch Gradient Descent）：在下山之前掌握了一部分的地势情况，选择总体平均梯度最小的方向下。准确度比 BGD 算法低，速度比 BGD 算法快。")]),a._v(" "),v("li",[a._v("动量法（Momentum）：在下山的时候带上了加速度，下山越来越快。速度会因为加速度变得更快，准确度较低，因为加速度导致刹不住脚从而容易错过山底跑到另一个山坡上。")]),a._v(" "),v("li",[a._v("牛顿加速梯度法（NAG, Nesterov Accelerated Gradient）：在下山的时候带上了加速度，同时时刻观察是否快到山底，然后快到山底的时候把速度降下来。速度比 Momentum 慢，准确度比 Momentum 高。")])]),a._v(" "),v("h3",{attrs:{id:"梯度"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#梯度"}},[a._v("#")]),a._v(" 梯度")]),a._v(" "),v("p",[a._v("函数在某一点处的梯度是这样的一个向量（矢量），它的方向是最大方向导数的方向，它的值是最大方向导数的值。")]),a._v(" "),v("h3",{attrs:{id:"学习率"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#学习率"}},[a._v("#")]),a._v(" 学习率")]),a._v(" "),v("p",[a._v("与优化器有关，决定学习速度，学习率（加速度）越大，则速度越快，同时也会更加容易错过极值点。")]),a._v(" "),v("h3",{attrs:{id:"二分类"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#二分类"}},[a._v("#")]),a._v(" 二分类")]),a._v(" "),v("p",[a._v("给定一个样本作为输入，输出的结果只能有两个（Yes/No、1/0、是/否）。")]),a._v(" "),v("p",[a._v("比如是否是垃圾邮件、是否是肿瘤、是否是癌症等。")]),a._v(" "),v("h3",{attrs:{id:"激活函数-activation-function"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#激活函数-activation-function"}},[a._v("#")]),a._v(" 激活函数 （Activation Function）")]),a._v(" "),v("p",[a._v("是神经网络模型中的神经元，它接受上层神经元的输出，其实也就成为了它的输入，把它的输入进行求和后，通过它的激活函数计算出结果，再输出给下层神经元，这样神经网络模型的表达能力就更加强大。")]),a._v(" "),v("ul",[v("li",[a._v("Sigmoid")]),a._v(" "),v("li",[a._v("Softmax")]),a._v(" "),v("li",[a._v("Tanh")]),a._v(" "),v("li",[a._v("ReLU")]),a._v(" "),v("li",[a._v("PReLU")]),a._v(" "),v("li",[a._v("ELU")])]),a._v(" "),v("h3",{attrs:{id:"广义线性模型"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#广义线性模型"}},[a._v("#")]),a._v(" 广义线性模型")]),a._v(" "),v("p",[a._v("研究变量之间的关系。")]),a._v(" "),v("h4",{attrs:{id:"线性回归-监督学习"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#线性回归-监督学习"}},[a._v("#")]),a._v(" 线性回归（监督学习）")]),a._v(" "),v("p",[a._v("用于对连续值的预测，比如身高和体重的关系，根据身高预测体重等。")]),a._v(" "),v("p",[a._v("模型：f(x) = wx + b")]),a._v(" "),v("h4",{attrs:{id:"非线性回归-监督学习"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#非线性回归-监督学习"}},[a._v("#")]),a._v(" 非线性回归（监督学习）")]),a._v(" "),v("p",[a._v("非线性回归模型与线性回归模型原理基本一样，区别在于线性回归模型是直线，非线性回归模型是曲线。")]),a._v(" "),v("p",[a._v("模型：f(x) = w1x + w2x^2 + ... + b")]),a._v(" "),v("h4",{attrs:{id:"逻辑回归-监督学习"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#逻辑回归-监督学习"}},[a._v("#")]),a._v(" 逻辑回归（监督学习）")]),a._v(" "),v("p",[a._v("用于对离散值的预测，比如二分类问题、多分类问题等。")]),a._v(" "),v("p",[a._v("模型：")]),a._v(" "),v("ul",[v("li",[a._v("Sigmoid：可用来解决二分类问题。")]),a._v(" "),v("li",[a._v("Softmax：可用来解决二分类、多分类问题。")]),a._v(" "),v("li",[a._v("K 近邻（KNN, K-Nearest Neighbor）：可用来解决二分类、多分类问题。")])]),a._v(" "),v("h3",{attrs:{id:"神经网络模型-nn-neural-network-人工神经网络模型-ann-artificial-neural-network"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#神经网络模型-nn-neural-network-人工神经网络模型-ann-artificial-neural-network"}},[a._v("#")]),a._v(" 神经网络模型（NN, Neural Network）/ 人工神经网络模型（ANN, Artificial Neural Network）")]),a._v(" "),v("p",[a._v("具备多层结构，1 个输入层（无计算逻辑，直接传递数据），0 个或多个隐含层（有计算逻辑，计算层），1 个输出层（有计算逻辑，计算层），每层由 1 个或多个神经元组成，而一个神经元就是一个函数（激活函数）。")]),a._v(" "),v("p",[a._v("输入层没有计算逻辑，是因为输入层的神经元没有激活函数，或者说它的激活函数是线性函数 f(x) = x。")]),a._v(" "),v("p",[a._v("输入层的神经元个数与特征数一致，输出层的神经元个数与标签数一致，隐含层的神经元个数根据设计者的经验决定。")]),a._v(" "),v("blockquote",[v("p",[a._v("因为神经网络模型是根据线性模型发展出来的，所以也可以用来解决线性回归的问题，实现 f(x) = wx + b。")]),a._v(" "),v("p",[a._v("实现方法：用 1 个输入层加上 1 个输出层，每层 1 个神经元，并且把输出层的激活函数去掉。")])]),a._v(" "),v("h4",{attrs:{id:"单层神经网络-监督学习"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#单层神经网络-监督学习"}},[a._v("#")]),a._v(" 单层神经网络（监督学习）")]),a._v(" "),v("p",[a._v("1 个输入层，0 个隐含层，1 个输出层。")]),a._v(" "),v("h4",{attrs:{id:"两层神经网络-监督学习"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#两层神经网络-监督学习"}},[a._v("#")]),a._v(" 两层神经网络（监督学习）")]),a._v(" "),v("p",[a._v("1 个输入层，1 个隐含层，1 个输出层。")]),a._v(" "),v("h4",{attrs:{id:"多层神经网络-深度学习-三层或三层以上"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#多层神经网络-深度学习-三层或三层以上"}},[a._v("#")]),a._v(" 多层神经网络（深度学习）（三层或三层以上）")]),a._v(" "),v("p",[a._v("1 个输入层，2+ 个隐含层，1 个 输出层。")]),a._v(" "),v("ul",[v("li",[a._v("深度神经网络（DNN, Deep Neural Network）（监督学习）：神经元全连接，容易造成梯度消失。")]),a._v(" "),v("li",[a._v("卷积神经网络（CNN, Convolutional Neural Networks）（监督学习）：适合空间矩阵数据，比如图像识别、人脸识别等。")]),a._v(" "),v("li",[a._v("循环神经网络（RNN, Recurrent Neural Network）（监督学习）：适合时间序列数据，比如语音识别、文本翻译、股价预测等。")])]),a._v(" "),v("h3",{attrs:{id:"生成式对抗网络模型-gan-generative-adversarial-networks-深度学习-无监督学习"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#生成式对抗网络模型-gan-generative-adversarial-networks-深度学习-无监督学习"}},[a._v("#")]),a._v(" 生成式对抗网络模型（GAN, Generative Adversarial Networks）（深度学习 / 无监督学习）")]),a._v(" "),v("p",[a._v("生成器和鉴别器的互相博弈学习产生相当好的输出。")]),a._v(" "),v("p",[a._v("比如，可用来生成高仿真的以假乱真的数据（图片、语音、视频、文字等）。")])])}),[],!1,null,null,null);t.default=r.exports}}]);